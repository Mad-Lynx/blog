---
title: Enhance performance by 99.9%
comments: true
tags:
  - performance
  - C#
  - additive inverse
---


I always liked challenges and impossible tasks.
E.g. generate a report for monthly sale, with aggregation of more than 1mln receipts and thousands of other transactions, on a very old PC - in less than 30 seconds.

So here we are again, new job, new problems, new challenges...

<!--more-->

* TOC
{:toc}

# Get familiar with the task

> Mark pairs of values which sum to zero.

So it's simply: "Find additive inverse of a number".  
The algorithm was very easy: go through all values and check if there is the opposite one. If so, mark both as a pair.

This was legacy code, so someone did the job before and I had to "just" improve the performance of it. So the code looked like this

```c#
var matched = new List<int>();
var pairs = new List<Pair>();
foreach (var element in elements)
{
    if (matched.Contains(element.Id))
        continue;

    foreach (var potentialPair in elements)
    {
        if (matched.Contains(potentialPair.Id))
            continue;

        if (element.Value == -potentialPair.Value)
        {
            matched.Add(element.Id);
            matched.Add(potentialPair.Id);

            pairs.Add(new Pair
            {
                Original = element,
                Inverse = potentialPair,
            });

            break;
        }
    }
}
```

# O(n^2) - this can't be good

## Two loops

Two nested loops - this mean _O(n^2)_. Eww.  
But apart from this, the code looks OK at first glance.  
However, when we change `foreach` to normal `for`:

```c#
foreach (var element in elements)
{
    // do something
}

for (var i = 0; i < elements.Count; i++)
{
    var element = elements[i];
    // do something
}
```

We can easily see, that the second loop iterates from the beginning.
This means, even for the second to last item (in the first loop) we will check **every** element in the second loop.  
But this is pointless, because we already checked the previous items. So we only need to check the items after "us".

```c#
for (var j = i + 1; j < elements.Count; j++)
```

<a data-collapsible="#why_loops" class="collapsible">Find out why...</a>

> ![Looking back]({{ site.url_img }}speedup/looking_back.png)
> Element `300` was already checked for additive inverse, in previous passes. If it was not marked then, it will not be marked now.  
> If `x + y != 0`, then `y + x != 0`.
{: .box .collapsed id="why_loops"}

## If collection contains

Some of you probably spotted that we save the matched ids in a `List<int>`. This is not a very efficient way to do this kind of operation.  
`List.Contains` is _O(n)_!! So this will mean, that we are dealing with an algorithm that is **O(n^3)** - whoops...

In cases like this, when you want to just add an element to the list and check if it exists or not, you should use `HashSet<>` - with is _O(1)_.

## Improved code

```c#
var matched = new HashSet<int>();
var pairs = new List<Pair>();
for (var i = 0; i < elements.Count; i++)
{
    var element = elements[i];
    if (matched.Contains(element.Id))
        continue;

    for (var j = i + 1; j < elements.Count; j++)
    {
        var potentialPair = elements[j];
        if (matched.Contains(potentialPair.Id))
            continue;

        if (element.Value == -potentialPair.Value)
        {
            matched.Add(element.Id);
            matched.Add(potentialPair.Id);

            pairs.Add(new Pair
            {
                Original = element,
                Inverse = potentialPair,
            });

            break;
        }
    }
}
```

OK, this code is **much** faster - but we are still dealing with _O(n^2)_.  
<a data-collapsible="#why_on2" class="collapsible">Find out why...</a>

> Some people (including me a long time ago), could think - why _O(n^2)_? If we narrowed the second loop and each time we will have less items,
> shouldn't be _O(n*log(n))_?  
> In fact we shortened time _only_ by half. You can see it, in [benchmark](#benchmark) paragraph.
> 
> You can watch [this detailed explanation](https://www.youtube.com/watch?v=4XkHbNi1ZL4){:target="_blank"} why we didn't change time complexity.
{: .box .collapsed id="why_on2"}

# The goal is O(n)

The only way I could achieve _O(n)_, is by removing the second loop. This means replacing it with something _O(1)_.

Some sort of `HashTable` comes into my mind. But I remembered: "don't use decimal/float as a key". But, **why** shouldn't we use it?

## Why floating point value is bad as a key?

Basically, you cannot trust that one value `1.0m` will be equal to other `1.0m`.
This is because these value can have [different precisions](http://stackoverflow.com/questions/2714645/decimal-truncate-trailing-zeros){:target="_blank"},
meaning the bit representation is different.  
But they should be equal and the HashCode should be the same.

Moreover, if you make some calculation you can lose some important information (again - precision).

```c#
var y = 49;
var x = 1.0m;
x = x / y;
x = x * y;

Console.WriteLine(x == 1.0m); // Oops!
```

In our scenario, data was loaded from DB and there was no operation (calculation) done, so every value should have the same precision.  
If you are not sure about precision in your case, just normalise your data before passing it to the method.

I was certain about precision, so I could simply bend the rule and use decimals as a key.

## Additive inverse detection

There are two different ways to _detect_ the inverse value if we put them in a `HashTable`:

1. search for the _current_ value, if not found then save the _inverse_ value
2. search for the _inverse_ value, if not found then save the _current_ value

I chose the first option, because of micro-optimization: if we found the current value, then we don't have to calculate the inverse - one less operation for the processor.

## List of candidates

We will have to know about "unused/unmatched" values, so we have to have some structure to keep this information.

Two structures are perfect for this: `Queue` and `Stack`. It doesn't matter which we choose - if we don't care about which element will be paired
(first with second or first with last).

<a data-collapsible="#candidates_comparison" class="collapsible">Compare...</a>

> ![Candidates]({{ site.url_img }}speedup/candidates.png)
{: .box .collapsed id="candidates_comparison"}



# New algorithm

Combining all the thoughts from above, I came up with something like this:

```c#
var candidates = new Dictionary<decimal, Queue<Element>>();
var pairs = new List<Pair>();
foreach (var element in elements)
{
    Queue<Element> queue;
    if (!candidates.TryGetValue(element.Value, out queue)
        || queue.Count == 0)
    {
        var inverse = Decimal.Negate(element.Value);
        if (!candidates.TryGetValue(inverse, out queue))
        {
            queue = new Queue<Element>();
            candidates.Add(inverse, queue);
        }

        queue.Enqueue(element);
    }
    else
    {
        var matched = queue.Dequeue();

        pairs.Add(new Pair
        {
            Original = matched,
            Inverse = element,
        });
    }
}
```

Of course this is not ideal and probably not the best solution.  
Let me know in comments or send a mail, about your ideas how to improve this.

# New (better) algorithm
_(update 2017-04-05)_

I knew that after publishing this post, there would be some attempts to beat it and make it more effective.

One of these attempts was made be my colleague, although his solution was slower - it was gentler with memory.  
He pointed out that we can use the absolute value as a key, because there will be no `-x` and `x` at the same time.
We will have all negative or all positive values for specific candidate.

Based on his suggestion, I tweaked my solution a little. The result - less memory and more speed.  
Thanks Jacob.

```c#
var candidates = new Dictionary<decimal, Queue<Element>>();
var pairs = new List<Pair>();
foreach (var element in elements)
{
    var elementValue = Math.Abs(element.Value);

    Queue<Element> queue;
    if (!candidates.TryGetValue(elementValue, out queue))
    {
        queue = new Queue<Element>();
        candidates.Add(elementValue, queue);
    }

    if (queue.Count == 0
        || queue.Peek().Value != -element.Value)
    {
        queue.Enqueue(element);
    }
    else
    {
        var matched = queue.Dequeue();

        pairs.Add(new Pair
        {
            Original = matched,
            Inverse = element,
        });
    }
}
```

# Benchmark

So now it's time to check if it was worth it or not.  
_You can download this little benchmark application from [here](https://gist.github.com/Mad-Lynx/49cf3c7f00c5e86f073dffe507efab99)._

|          | Elements | Pairs  |     Time     |
| :---     |     ---: |   ---: |    :----:    |
| Original |   10,000 |   1613 | 00:04:44.18s |
| HashSet  |   10,000 |   1613 | 00:00:04.49s |
| Improved |   10,000 |   1612 | 00:00:02.24s |
| New      |   10,000 |   1612 | **00:00:00.007s** |
| Better   |   10,000 |   1612 | **00:00:00.002s** |
|          |          |        |              |
| Original |   50,000 |   5005 | **06:47:29.07s** |
| HashSet  |   50,000 |   5005 | 00:02:07.32s |
| Improved |   50,000 |   5005 | 00:01:03.45s |
| New      |   50,000 |   5005 | **00:00:00.023s** |
| Better   |   50,000 |   5005 | **00:00:00.014s** |
|          |          |        |              |
| Original |  200,000 |  _???_ | _(probably days)_ |
| HashSet  |  200,000 |  16591 | 00:43:03.35s |
| Improved |  200,000 |  16591 | 00:21:01.69s |
| New      |  200,000 |  16591 | **00:00:00.122s** |
| Better   |  200,000 |  16591 | **00:00:00.114s** |


<a data-collapsible="#legend" class="collapsible">Legend</a>

Original
:    original code

HashSet
:    after changing to `HashSet`

Improved
:    after changing to smaller loops

New
:    totally new algorithm

Better
:    new algorithm after tweaks
{: .box .collapsed id="legend"}

# Remarks

## Bug

While I was preparing some benchmarks, I noticed a difference between the lists returned by the new and old methods.

This is because the original method, if the value was `0`, returned the element as a pair with `Original` and `Inverse` pointing to the same instance.  
And because in our "improved" version we search from current position ahead (`j = i + 1`), the algorithm will pick up only "pairs" of zeros - always different references.

If you feel that the previous behaviour was desirable, you can easily change it - by checking if the value is zero at the beginning of the main loop.

In my case there was no point in checking, because the data didn't contain values of `0`.

## Memory

The biggest issue we now have is large memory-usage.  
Because now we are tracking rows that are not matched (yet), if the input data contains only a few pairs, we have to keep references to a large number of objects.

For test data of 1M rows, where each row has a different positive value, we have to allocate 170MB.

We could use a different type to store `candidates`, e.g. `Stack`, but still it will not make a big difference for widespread values.

<a data-collapsible="#type_size" class="collapsible">Size of an instance...</a>

> | Type            | (empty) |   1  |   2  |   5  |   8  |
> | :---            |    ---: | ---: | ---: | ---: | ---: |
> | `List<T>`       |      24 |  100 |  100 |  166 |  166 |
> | `Stack<T>`      |      24 |  100 |  100 |  166 |  166 |
> | `Queue<T>`      |      32 |  108 |  108 |  174 |  174 |
> | `LinkedList<T>` |      28 |   64 |  100 |  208 |  317 |
>
> The values represent how many bytes one instance of the object takes up in memory.  
> Columns _1-8_ show how many items (`decimal` values) have been added to the object.  
{: .box .collapsed id="type_size"}
